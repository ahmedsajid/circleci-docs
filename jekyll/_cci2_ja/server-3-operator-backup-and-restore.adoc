---
version:
- Server v3.x
- Server Admin
---
= CircleCI Server v3.x Backup and Restore
:page-layout: classic-docs
:imagesdir: ../assets/img/docs/
:page-liquid:
:page-description: This document outlines recommendations for how to back up and restore your CircleCI server instance data and state.
:icons: font
:toc: macro
:toc-title:

toc::[]

== 概要
_バックアップと復元は、CircleCI Server v3.1.0 以上で利用できます。_

While operating and administering CircleCI server, you will consider how to maintain backups and recover your installation, should there be a need to migrate it to another cluster or recover from a critical event.
このドキュメントでは、CircleCI Server インスタンスのデータと状態のバックアップを行い復元する方法に関する推奨事項を説明します。

CircleCI server is administered via https://kots.io/[KOTS], which uses https://velero.io/[Velero] for backup and restore.
この方法のメリットは、アプリケーションのデータだけでなく、バックアップ時点の Kubernetes クラスタの状態とリソースも復元することです。
それにより、管理者コンソールの設定や、クラスタに加えたカスタマイズ内容も復元することができます。 

NOTE: CircleCI サービスのバックアップと復元は、Velero に依存しています。 If your cluster is lost, you will not be able to restore CircleCI until you have successfully started Velero in the cluster. Velero が正常に起動すれば、CircleCI サービスを復元できます。

== セットアップ

Backups of CircleCI server can be created quite easily through https://kots.io/[KOTS].
ただし、バックアップ サポートを有効にするには、クラスタに https://velero.io/[Velero]をインストールして設定する必要があります。
以下のセクションでは、クラスタへの Velero のインストール手順を説明します。

=== 前提条件

- お使いの環境に合った https://velero.io/docs/v1.6/basic-install/[Velero CLI] をダウンロードしてインストールします。

==== AWS を使う場合の前提条件

- https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html[AWS CLI] をインストール済みであること。

==== GCP を使う場合の前提条件

- `gcloud` と `gsutil` をインストール済みであること。 これらは両方とも Google Cloud SDK に含まれており、SDK をインストールすることでセットアップできます (https://cloud.google.com/sdk/docs/[こちらのドキュメント]を参照してください)。

詳細については、Velero のhttps://velero.io/docs/v1.6/supported-providers/[[サポート対象プロバイダーに関するページ]]を参照してください。

以下で、AWS と GCP のそれぞれで CircleCI Server 3.x のバックアップを作成する手順について説明します。

==== S3 互換ストレージを使う場合の前提条件

- お使いのストレージ プロバイダーに合った https://docs.min.io/docs/minio-client-quickstart-guide.html[minio CLI] をインストールし設定済みであること。

////

* AWS SETUP *

////
== AWS での CircleCI Server 3.x のバックアップ

以下の手順では、プロバイダーが AWS であり、上記の<<prerequisites, 前提条件>>を満たしていることを前提としています。

これらの手順は、https://github.com/vmware-tanzu/velero-plugin-for-aws#setup[こちら]の Velero ドキュメントを元にしています。

=== 手順 1 - AWS S3 バケットの作成

[source,bash]
----
BUCKET=<YOUR_BUCKET>
REGION=<YOUR_REGION>
aws s3api create-bucket \
    --bucket $BUCKET \
    --region $REGION \
    --create-bucket-configuration LocationConstraint=$REGION
----
NOTE: `us-east-1` では、https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateBucket.html#API_CreateBucket_RequestBody[[LocationConstraint]] がサポートされていません。 `us-east-1` リージョンを使用している場合、バケットの設定は省略してください。

=== 手順 2 - Velero の権限の設定

* IAM ユーザーを作成します。

[source,bash]
----
aws iam create-user --user-name velero
----

* 必要な権限を付与するポリシーをユーザー `velero` にアタッチします。

[source,bash]
----
cat > velero-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}"
            ]
        }
    ]
}
EOF
----

[source,bash]
----
aws iam put-user-policy \
  --user-name velero \
  --policy-name velero \
  --policy-document file://velero-policy.json
----

* Create an access key for user `velero`:

[source,bash]
----
aws iam create-access-key --user-name velero
----

このコマンドの結果は以下のようになります。
[source,json]
----
{
  "AccessKey": {
        "UserName": "velero",
        "Status": "Active",
        "CreateDate": "2017-07-31T22:24:41.576Z",
        "SecretAccessKey": <AWS_SECRET_ACCESS_KEY>,
        "AccessKeyId": <AWS_ACCESS_KEY_ID>
  }
}
----

* Create a Velero-specific credentials file (for example, `./credentials-velero`) in your local directory, with the following contents:

[source,bash]
----
[default]
aws_access_key_id=<AWS_ACCESS_KEY_ID>
aws_secret_access_key=<AWS_SECRET_ACCESS_KEY>
----
ここで、`AWS_ACCESS_KEY_ID` プレースホルダーと `AWS_SECRET_ACCESS_KEY` プレースホルダーには、前の手順の `create-access-key` リクエストで返された値を指定します。

=== 手順 3 - Velero のインストールと起動

* 以下の `velero` `install` コマンドを実行します。 これにより、`velero` という名前空間が作成され、Velero を実行するのに必要なリソースがすべてインストールされます。
Make sure that you pass the correct file name containing the AWS credentials that you have created in <<Step 2 - Setup permissions for Velero, Step 2>>.

NOTE: KOTS backups require https://restic.net/[restic] to operate. Velero のインストール時に、以下に示すように `--use-restic` フラグを設定してください。

[source, bash]
----
velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.2.0 \
    --bucket $BUCKET \
    --backup-location-config region=$REGION \
    --snapshot-location-config region=$REGION \
    --secret-file ./credentials-velero \
    --use-restic \
    --wait
----

* Velero がクラスタにインストールされたら、新しい `velero` 名前空間を確認します。 You should have a Velero deployment and a restic daemonset, for example:

[source,bash]
----
$ kubectl get pods --namespace velero
NAME                      READY   STATUS    RESTARTS   AGE
restic-5vlww              1/1     Running   0          2m
restic-94ptv              1/1     Running   0          2m
restic-ch6m9              1/1     Running   0          2m
restic-mknws              1/1     Running   0          2m
velero-68788b675c-dm2s7   1/1     Running   0          2m
----

restic はデーモンセットなので、Kubernetes クラスタ内のノード 1 つにつき 1 つの Pod が存在します。

////

* GCP SETUP *

////
== GCP での CircleCI Server 3.x のバックアップ

以下の手順は、Google Cloud Platform を対象としており、<<prerequisites, 前提条件>>を満たしていることを前提としています。

これらの手順は、https://github.com/vmware-tanzu/velero-plugin-for-gcp#setup[[こちら]]の Velero GCP プラグインのドキュメントを元にしています。

=== 手順 1 - GCP バケットの作成
To reduce the risk of typos, we will set some of the parameters as shell variables. If you are unable to complete all the steps in the same session, do not forget to reset variables as necessary before proceeding. たとえば、以下の手順では、バケット名に対応する変数を定義しています。 Replace the `<YOUR_BUCKET>` placeholder with the name of the bucket you want to create for your backups.

[source,bash]
----
BUCKET=<YOUR_BUCKET>

gsutil mb gs://$BUCKET/
----

=== 手順 2 - Velero の権限の設定

CircleCI Server を GKE クラスタ内で実行している場合、RBAC オブジェクトを作成する必要があるため、使用する IAM ユーザーをクラスタの管理者に設定してください。 詳細については、https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control#iam-rolebinding-bootstrap[[GKE のドキュメント]]を参照してください。

. 最初に、プロジェクト ID に対応するシェル変数を設定します。 それにはまず、次のコマンドを実行して現在の設定を調査し、`gcloud` CLI が正しいプロジェクトを参照していることを確認します。
+
[source,bash]
----
gcloud config list
----

. If the project is correct, set the variable as follows:
+
[source,bash]
----
PROJECT_ID=$(gcloud config get-value project)
----

. 以下のコマンドを実行して、サービス アカウントを作成します。
+
[source,bash]
----
gcloud iam service-accounts create velero \
    --display-name "Velero service account"
----
NOTE: If you run several clusters with Velero, you might want to consider using a more specific name for the Service Account besides `velero`, as suggested in the example above.

. 以下のコマンドを実行して、サービス アカウントが正常に作成されたことを確認します。
+
[source,bash]
----
gcloud iam service-accounts list
----

. 次に、サービス アカウントの電子メール アドレスを変数に格納します。
+
[source,bash]
----
SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list \
  --filter="displayName:Velero service account" \
  --format 'value(email)')
----
サービス アカウントに付けた表示名に合わせて、必要に応じてコマンドを変更してください。

. 必要な権限をサービス アカウントに付与します。
+
[source,bash]
----
ROLE_PERMISSIONS=(
    compute.disks.get
    compute.disks.create
    compute.disks.createSnapshot
    compute.snapshots.get
    compute.snapshots.create
    compute.snapshots.useReadOnly
    compute.snapshots.delete
    compute.zones.get
)

gcloud iam roles create velero.server \
    --project $PROJECT_ID \
    --title "Velero Server" \
    --permissions "$(IFS=","; echo "${ROLE_PERMISSIONS[*]}")"

gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member serviceAccount:$SERVICE_ACCOUNT_EMAIL \
    --role projects/$PROJECT_ID/roles/velero.server

gsutil iam ch serviceAccount:$SERVICE_ACCOUNT_EMAIL:objectAdmin gs://${BUCKET}
----

次に、Velero でこのサービス アカウントを使用できるようにする必要があります。

==== オプション 1: JSON キー ファイル

サービス アカウントとしてアクションを実行できるように Velero を認証するには、JSON 認証情報ファイルを Velero に渡します。 それにはまず、以下のコマンドを実行してキーを作成します。
[source,bash]
----
gcloud iam service-accounts keys create credentials-velero \
    --iam-account $SERVICE_ACCOUNT_EMAIL
----
After running this command, you should see a file named `credentials-velero` in your local working directory.

==== オプション 2: Workload Identity

クラスタで既に https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity[Workload Identity] を使用している場合は、先ほど作成した GCP サービス アカウントを Velero の Kubernetes サービス アカウントにバインドします。 この場合、GCP サービス アカウントには、上記で指定済みの権限に加え、`iam.serviceAccounts.signBlob` ロールも必要です。

=== 手順 3 - Velero のインストールと起動

* サービス アカウントの認証方法に応じて、以下の `velero` `install` コマンドのいずれかを実行します。 これにより、`velero` という名前空間が作成され、Velero を実行するのに必要なリソースがすべてインストールされます。

NOTE: KOTS のバックアップを使用するには、https://restic.net/[restic] が必要です。 Velero のインストール時に、`--use-restic` フラグを設定してください。

==== JSON キー ファイルを使用する場合

[source, bash]
----
velero install \
    --provider gcp \
    --plugins velero/velero-plugin-for-gcp:v1.2.0 \
    --bucket $BUCKET \
    --secret-file ./credentials-velero \
    --use-restic \
    --wait
----

==== Workload Identity を使用する場合

[source,bash]
----
velero install \
    --provider gcp \
    --plugins velero/velero-plugin-for-gcp:v1.2.0 \
    --bucket $BUCKET \
    --no-secret \
    --sa-annotations iam.gke.io/gcp-service-account=$SERVICE_ACCOUNT_EMAIL \
    --backup-location-config serviceAccount=$SERVICE_ACCOUNT_EMAIL \
    --use-restic \
    --wait
----

システムをカスタマイズする他のオプションについては、https://github.com/vmware-tanzu/velero-plugin-for-gcp#install-and-start-velero[[Velero のドキュメント]]を参照してください。

* Velero がクラスタにインストールされたら、新しい `velero` 名前空間を確認します。 You should have a Velero deployment and a restic daemonset, for example:

[source,bash]
----
$ kubectl get pods --namespace velero
NAME                      READY   STATUS    RESTARTS   AGE
restic-5vlww              1/1     Running   0          2m
restic-94ptv              1/1     Running   0          2m
restic-ch6m9              1/1     Running   0          2m
restic-mknws              1/1     Running   0          2m
velero-68788b675c-dm2s7   1/1     Running   0          2m
----

restic はデーモンセットなので、Kubernetes クラスタ内のノード 1 つにつき 1 つの Pod が存在します。

////

* S3-COMPATIBLE SETUP *

////
== S3 互換ストレージでの CircleCI Server 3.x のバックアップ

The following steps will assume you arre using S3-compatible object storage, but not necessarily AWS S3, for your backups.
また、<<s3-compatible-storage-prerequisites, 前提条件>>を満たしていることも前提としています。

これらの手順は、https://velero.io/docs/v1.6/contributions/minio/[こちら] の Velero ドキュメントを元にしています。

=== 手順 1 - `mc` クライアントの設定

最初に、ストレージ プロバイダーに接続できるよう https://docs.min.io/minio/baremetal/reference/minio-cli/minio-mc.html[`mc`] を設定します。

[source,bash]
----
# エイリアスは任意の名前でかまいませんが、以降のコマンドでも同じ値を使用してください。
export ALIAS=my-provider
mc alias set $ALIAS <YOUR_MINIO_ENDPOINT> <YOUR_MINIO_ACCESS_KEY_ID> <YOUR_MINIO_SECRET_ACCESS_KEY>
----

クライアントが適切に設定されたかどうかは、`mc ls my-provider` を実行して確認できます。

=== 手順 2 - バケットの作成

バックアップ用のバケットを作成します。 It is important that a new bucket is used, as Velero cannot use an existing bucket that already contains other content.

[source, bash]
----
mc mb ${ALIAS}/<YOUR_BUCKET>
----

=== 手順 3 - ユーザーとポリシーの作成

次に、Velero がバケットにアクセスするためのユーザーとポリシーを作成します。

NOTE: In the following snippet `<YOUR_MINIO_ACCESS_KEY_ID>` and `<YOUR_MINIO_SECRET_ACCESS_KEY>` refer to the credentials used by Velero to access MinIO.

[source, bash]
----
# ユーザーを作成します。
mc admin user add $ALIAS <YOUR_MINIO_ACCESS_KEY_ID> <YOUR_MINIO_SECRET_ACCESS_KEY>

# ポリシーを作成します。
cat > velero-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:*"
      ],
      "Resource": [
        "arn:aws:s3:::<YOUR_BUCKET>",
        "arn:aws:s3:::<YOUR_BUCKET>/*"
      ]
    }
  ]
}
EOF

mc admin policy add $ALIAS velero-policy velero-policy.json

# ユーザーをポリシーにバインド
mc admin policy set $ALIAS velero-policy user=<YOUR_VELERO_ACCESS_KEY_ID>
----

最後に、新しいユーザーの認証情報を以下の形式で記述したファイルを作成します (この例では `./credentials-velero`)。

[source,toml]
----
[default]
aws_access_key_id=<YOUR_VELERO_ACCESS_KEY_ID>
aws_secret_access_key=<YOUR_VELERO_SECRET_ACCESS_KEY>
----

=== 手順 4 - Velero のインストールと起動

以下の `velero install` コマンドを実行します。 これにより、`velero` という名前空間が作成され、Velero を実行するのに必要なリソースがすべてインストールされます。

NOTE: KOTS backups require https://restic.net/[restic] to operate. Velero のインストール時に、以下に示すように `--use-restic` フラグセットを設定してください。

[source, bash]
----
velero install --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.2.0 \
  --bucket <YOUR_BUCKET> \
  --secret-file ./credentials-velero \
  --use-volume-snapshots=false \
  --use-restic \
  --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=<YOUR_ENDPOINT> \
  --wait
----

Velero がクラスタにインストールされたら、新しい `velero` 名前空間を確認します。 You
should have a Velero deployment and a restic daemonset, for example:

[source,bash]
----
$ kubectl get pods --namespace velero
NAME                      READY   STATUS    RESTARTS   AGE
restic-5vlww              1/1     Running   0          2m
restic-94ptv              1/1     Running   0          2m
restic-ch6m9              1/1     Running   0          2m
restic-mknws              1/1     Running   0          2m
velero-68788b675c-dm2s7   1/1     Running   0          2m
----

restic はデーモンセットなので、Kubernetes クラスタ内のノード 1 つにつき 1 つの Pod が存在します。

== バックアップの作成
Now that Velero is installed on your cluster, you should see the **Snapshots** option in the navbar of the management console.

image::kots-admin-navbar-snapshot-option.png[Kots Navbar]

このオプションが表示されれば、バックアップの作成を始める準備は完了です。 このオプションが表示されない場合は、<<troubleshooting-backups-and-restoration, トラブルシューティングに関するセクション>>を参照してください。

=== Option 1 - Create a backup with KOTS CLI

バックアップを作成するには、以下のコマンドを実行します。

[source,bash]
----
kubectl kots backup --namespace <your namespace>
----

=== Option 2 - Create a backup with KOTS Admin Console

ナビゲーション バーの *[Snapshots (スナップショット)]* を選択します。 デフォルトでは *[Full Snapshots (フル スナップショット)]* が選択されています。 これが推奨オプションです。

image::kots-admin-full-snapshot.png[Kots Navbar]

Click the *Start a snapshot* button.

image::kots-admin-create-backup.png[Kots Create Snapshot]

== バックアップの復元

=== オプション 1 - スナップショットからのバックアップ復元

S3 互換ストレージに保存済みのバックアップから復元するには、Kubernetes クラスタに Velero がインストールされており、バックアップの格納されているストレージ バケットに Velero がアクセスできる必要があります。 When using EKS, restoring CircleCI server requires that an instance of CircleCI server is installed beforehand. GKE や他のプラットフォームを使用する場合は、クラスタに Velero さえインストールされていれば機能します。  

NOTE: If this is a new cluster or if you need to reinstall Velero, the installation should be done with the same credentials generated above.

=== Option 2 - Restore a backup using the KOTS CLI

To restore a backup using the KOTS CLI, run the following command to get a list of backups:

[source,bash]
----
kubectl kots get backups
----

復元プロセスを開始するには、前述のコマンドで取得したバックアップ名を使用して、以下のコマンドを実行します。

[source,bash]
----
kubectl kots restore --from-backup <backup-instance-id>
----

=== Option 3 - Restore a backup using the KOTS Admin Console UI

As with backups, navigate to *Snapshots* in the KOTS Admin Console. 今回は、復元アイコン付きのバックアップがすべて表示されます。
使用するバックアップを選択し、復元アイコンを選択します。
使用するバックアップを選択し、復元を選択します。

image::kots-admin-restore.png[Kots Create Snapshot]

IMPORTANT: 復元すると、CircleCI サービス用に新しいロード バランサーが作成されます。 You will need to either update your DNS records or the hostname configurations in KOTS Admin Console as a result. You may also need to consider updating the `nomad server endpoint` provided to your Nomad clients.

IMPORTANT: If you are using pre-existing Nomad clients, you will need to restart them before they will connect to the nomad-server cluster.

CircleCI Server が復元され、運用可能な状態になるまで、約 10 ～ 15 分かかります。

== オプション - KOTS 使用したバックアップのスケジュール設定

To schedule regular backups, select *Snapshots*, and then *Settings & Schedule* from the KOTS Admin Console.

image::kots-admin-scheduled-backup.png[Snapshots Selected]

And here you can find configurations related to your snapshots, including scheduling.

image::kots-admin-scheduled-snapshots.png[Snapshot Settings]

== バックアップと復元のトラブルシューティング

=== Snapshots are not available in KOTS Admin Console

If your KOTS Admin Console does not display the snapshot option, you may try the following:

* Confirm that your version of KOTS supports snapshots. 現時点では、v1.40.0 以上が推奨されます。

```shell
$ kubectl kots version
Replicated KOTS 1.40.0
```

* Velero がデプロイされ、適切に動作していることを確認します。 Velero のログは、以下のコマンドで確認できます。

```shell
$ kubectl logs deployment/velero --namespace velero
```

問題がある場合は、Velero の再インストールが必要です。

* お使いのライセンスでスナップショットを利用できることを確認します。 You may reach out to our Customer Support Team for confirmation.

=== バックアップ プロセスまたは復元プロセスでエラーが発生した

バックアップまたは復元プロセスでエラーが発生した場合は、まず Velero ログを確認してください。
上記のコマンドの結果 4XX エラーが見つかった場合、ストレージ バケットへのアクセスの問題が原因の可能性があります。

* バケットが存在していることと、想定するリージョンにあることを確認します。
* Confirm that the credentials provided to Velero can be used to access the bucket.
* You may need to run the command to install Velero again, this time with updated bucket information.

You may also check the status of pods in the `velero` namespace:

```
$ kubectl get pods --namespace velero
NAME                      READY   STATUS    RESTARTS   AGE
restic-5vlww              1/1     Pending   0          10m
restic-94ptv              1/1     Running   0          10m
restic-ch6m9              1/1     Pending   0          10m
restic-mknws              1/1     Running   0          10m
velero-68788b675c-dm2s7   1/1     Running   0          10m
```

In the above example, some restic pods are pending, which means they are waiting for a node to have available CPU or memory resources. In this case, you may need to scale your nodes to accommodate restic.
